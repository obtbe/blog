<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Skoltech ACS Â· interview prep</title>
    <style>
        /* minimal, classic, small font */
        body {
            background: #f8fafc;
            font-family: 'Courier New', 'Lucida Console', 'Liberation Mono', monospace;
            font-size: 13px;
            line-height: 1.5;
            margin: 2rem auto;
            max-width: 1000px;
            padding: 0 20px;
            color: #1e293b;
        }
        h1 {
            font-size: 1.6rem;
            font-weight: 600;
            border-bottom: 2px solid #334155;
            padding-bottom: 0.3rem;
            margin-top: 2rem;
            margin-bottom: 1.5rem;
            letter-spacing: -0.3px;
        }
        h2 {
            font-size: 1.3rem;
            font-weight: 600;
            margin-top: 2rem;
            margin-bottom: 0.75rem;
            background: #e9edf2;
            padding: 0.3rem 0.5rem;
            border-left: 5px solid #0f172a;
        }
        h3 {
            font-size: 1.1rem;
            font-weight: 600;
            margin: 1.5rem 0 0.5rem;
            border-bottom: 1px dashed #64748b;
            padding-bottom: 0.2rem;
        }
        h4 {
            font-size: 0.95rem;
            font-weight: 600;
            margin: 1.2rem 0 0.2rem;
            color: #0f172a;
        }
        p, li {
            margin: 0.5rem 0;
            text-align: justify;
        }
        ul, ol {
            margin: 0.3rem 0 0.8rem 1.5rem;
            padding-left: 0;
        }
        li {
            margin: 0.2rem 0;
        }
        pre {
            background: #1e293b;
            color: #dee2e6;
            padding: 0.8rem 1rem;
            border-radius: 6px;
            overflow-x: auto;
            font-size: 0.9rem;
            font-family: 'Courier New', monospace;
            border: 1px solid #475569;
            margin: 1rem 0;
        }
        code {
            background: #e2e8f0;
            padding: 0.2rem 0.4rem;
            border-radius: 4px;
            font-size: 0.9rem;
            color: #0b1e33;
        }
        pre code {
            background: transparent;
            color: inherit;
            padding: 0;
        }
        hr {
            border: none;
            border-top: 2px dotted #94a3b8;
            margin: 1.8rem 0;
        }
        .note {
            background: #f1f5f9;
            border-left: 5px solid #475569;
            padding: 0.8rem 1.2rem;
            font-style: italic;
            margin: 1.5rem 0;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 1rem 0;
            font-size: 0.9rem;
        }
        th, td {
            border: 1px solid #b9c7da;
            padding: 0.4rem 0.6rem;
            text-align: left;
        }
        th {
            background: #d1dbe8;
            font-weight: 600;
        }
        .question {
            font-weight: 600;
            color: #022b46;
            margin-top: 1.2rem;
        }
        .section-tag {
            background: #0f172a;
            color: white;
            display: inline-block;
            padding: 0.1rem 0.6rem;
            font-size: 0.75rem;
            letter-spacing: 0.5px;
            margin-right: 0.5rem;
        }
    </style>
</head>
<body>

<h1>ğŸ“ Skoltech ACS Â· interview preparation</h1>
<p><strong>Advanced Computational Science Â· distributed graph analytics Â· supercomputing</strong></p>

<!-- ============ SECTION 1 ============ -->
<h2>SECTION 1 â€“ Motivation & fit</h2>

<h3>Q1: â€œWhy ACS over standard CS/DS?â€</h3>
<p><strong>Sample answer:</strong> I chose ACS because it sits exactly at the intersection of highâ€‘performance computing, applied mathematics, and largeâ€‘scale data analysis. While standard CS focuses on software engineering and DS on business analytics, ACS offers the computational depth to solve scientific problems at scale. I am drawn to parallel & distributed computing, numerical algorithms, and the opportunity to work on supercomputing architectures. Skoltechâ€™s projectâ€‘based model and worldâ€‘class facilities make it the ideal place to develop skills for my research interests.</p>

<h3>Q2: â€œWhich research areas interest you most?â€</h3>
<p><strong>Sample answer:</strong> I am particularly interested in <strong>Distributed Graph Analytics on Modern Supercomputing Architectures</strong>. Graphs appear everywhere â€” social networks, biology, transport â€” and as datasets grow to billions of nodes, we need algorithms that scale across distributed memory. Modern supercomputers add complexity: hybrid MPI+X, heterogeneous nodes (CPUs+GPUs), deep memory hierarchies. Designing graph algorithms that minimise communication and exploit these architectures is a crucial challenge. I follow frameworks like Dâ€‘Galois, Gemini, and MPIâ€‘based graph libraries, and I am also curious about scaling graph neural networks to distributed settings.</p>

<h3>Q3: â€œWhat do you want to work on for your masterâ€™s thesis?â€</h3>
<p><strong>Sample answer:</strong> Ideally, I would work on distributed graph algorithms for supercomputing. Three directions attract me:<br>
â€¢ <em>Communicationâ€‘avoiding graph algorithms</em> â€“ new partitioning or asynchronous models that trade redundant computation for less communication.<br>
â€¢ <em>Heterogeneous graph analytics</em> â€“ using both CPUs and GPUs, work distribution, and memory management for irregular graph structures.<br>
â€¢ <em>Scaling graph neural networks</em> â€“ distributed training, miniâ€‘batch sampling across partitions, and efficient gradient synchronisation.<br>
I am open to refining these ideas based on Skoltechâ€™s ongoing projects and would love to contribute to research in this area.</p>

<h3>Q4: â€œWhere do you see yourself in 5 years?â€</h3>
<p><strong>Sample answer:</strong> I see myself pursuing a PhD in computational science / computer science, focusing on highâ€‘performance computing and largeâ€‘scale graph analytics. The ACS programme will give me the mathematical and research foundation for doctoral studies. Longer term, I aim to develop exascale algorithms for realâ€‘world problems â€” scientific discovery, infrastructure optimisation, or working at a national laboratory. Alternatively, I could see myself in industrial R&amp;D, building productionâ€‘quality parallel graph frameworks. Both paths build on what I would learn at Skoltech.</p>

<hr>

<!-- ============ SECTION 2 ============ -->
<h2>SECTION 2 â€“ Technical fundamentals</h2>

<h3>â””â”€ A. Mathematics</h3>

<h4>Linear algebra</h4>
<p class="question">ğŸ”¹ Geometric interpretation of eigenvectors/eigenvalues</p>
<p><strong>Answer:</strong> An eigenvector is a direction preserved under a linear transformation â€” it gets scaled but not rotated. The eigenvalue tells how much scaling occurs: |Î»|&gt;1 stretches, |Î»|&lt;1 compresses, negative flips, zero collapses to null space. In graph theory, eigenvectors of the Laplacian (e.g. Fiedler vector) give spectral embeddings used for partitioning â€” directly relevant to distributed graph analytics. In PCA, eigenvectors of the covariance matrix point to directions of maximum variance, with eigenvalues indicating the variance along each component.</p>

<p class="question">ğŸ”¹ What is matrix decomposition? Why is SVD so important?</p>
<p><strong>Answer:</strong> A matrix decomposition factors a matrix into simpler pieces that reveal structure or simplify computations â€” like factoring an integer. SVD (A = UÎ£Váµ€) is fundamental: U and V are orthonormal bases, Î£ contains singular values (sorted). Importance: (1) best lowâ€‘rank approximation (PCA, compression), (2) reveals condition number, (3) stable pseudoinverse, (4) latent factors in recommendation, (5) spectral graph analysis (community detection via adjacency SVD). For my interests, SVD is key for partitioning and embedding graphs.</p>

<p class="question">ğŸ”¹ Condition number â€“ why does it matter?</p>
<p><strong>Answer:</strong> Îº(A) = Ïƒ_max/Ïƒ_min measures sensitivity of Ax=b to input perturbations. High Îº â†’ illâ€‘conditioned â†’ small rounding errors can explode. In numerical computing it affects: algorithm choice (direct vs iterative), need for preconditioning, trust in solutions. Graph Laplacians often become illâ€‘conditioned as graphs grow, influencing convergence of iterative solvers for spectral partitioning. Understanding Îº helps predict iteration counts and stability.</p>

<h4>Calculus & optimization</h4>
<p class="question">ğŸ”¹ Gradient descent, learning rate, stochastic GD</p>
<p><strong>Answer:</strong> Gradient descent updates parameters Î¸ â† Î¸ â€“ Î·âˆ‡J(Î¸) to minimise J. Learning rate Î· controls step size: too small â†’ slow, too large â†’ divergence/oscillation. SGD uses a single random sample (or miniâ€‘batch) per step, introducing noise. Advantages: speed, escapes shallow minima, online learning, implicit regularisation. In distributed settings, variants (sync/async, data/model parallelism) are used to scale training, e.g. for GNNs on massive graphs.</p>

<p class="question">ğŸ”¹ Lagrange multipliers â€“ concept & uses</p>
<p><strong>Answer:</strong> Method to optimise f(x) subject to g(x)=c. At optimum, âˆ‡f âˆ¥ âˆ‡g â‡’ âˆ‡f = Î»âˆ‡g. Î» = Lagrange multiplier, measures sensitivity of optimum to constraint relaxation. Used in SVM (dual formulation, support vectors), PCA (variance under orthogonality), graph partitioning (minâ€‘cut with balance â†’ eigenvalue problem), duality theory (distributed optimisation), and information theory (maxâ€‘ent distributions).</p>

<h4>Probability & statistics</h4>
<p class="question">ğŸ”¹ MLE vs MAP</p>
<p><strong>Answer:</strong> MLE finds Î¸ maximising P(data|Î¸) â€” no prior. MAP maximises posterior P(Î¸|data) âˆ P(data|Î¸)P(Î¸) â€” includes prior. MAP = MLE + regularisation (Gaussian prior â†’ L2, Laplace â†’ L1). In graph problems, priors can encode smoothness or community size expectations. As data grows, MAP converges to MLE.</p>

<p class="question">ğŸ”¹ Biasâ€‘variance tradeoff</p>
<p><strong>Answer:</strong> Expected error = biasÂ² + variance + irreducible noise. Bias: systematic error from simplistic assumptions (underfitting). Variance: sensitivity to training fluctuations (overfitting). Complex models lower bias but raise variance. In distributed graph analytics: GNN depth vs overâ€‘smoothing (bias), graph sampling (variance), communicationâ€‘reducing approximations (bias) to lower network variance. Crossâ€‘validation finds the sweet spot.</p>

<p class="question">ğŸ”¹ Central Limit Theorem â€“ why useful?</p>
<p><strong>Answer:</strong> CLT: for large samples, the distribution of the sample mean approaches normal, regardless of original distribution. Useful for: confidence intervals, hypothesis testing, Monte Carlo error bounds (error âˆ 1/âˆšn), performance analysis (mean of benchmark runs), distributed aggregation (aggregated stats become normal). In graph sampling, it justifies error estimates when estimating properties from sampled subgraphs.</p>

<h3>â””â”€ B. Computer science & algorithms</h3>

<p class="question">ğŸ”¹ O(n log n) vs O(nÂ²) â€“ examples</p>
<p><strong>Answer:</strong> O(n log n) grows much slower (for n=1e6: ~20M vs 1e12 operations). O(n log n) algorithms: merge sort, heap sort, FFT, Dijkstra (binary heap), many sparse graph algorithms (O((n+m)log n)). O(nÂ²) algorithms: bubble sort, naive matrix multiplication, Floydâ€‘Warshall, allâ€‘pairs checks. For massive graphs (billions of nodes), O(nÂ²) is impossible; we need nearâ€‘linear scaling in edges, plus communication awareness in distributed settings.</p>

<p class="question">ğŸ”¹ Hash table vs list â€“ when to use each</p>
<p><strong>Answer:</strong> Hash table: O(1) average lookups by key, uniqueness, set ops, counting, caching. List: when order matters, indexâ€‘based access, sequential iteration, memory constraints. In distributed graph frameworks, hybrid patterns appear: array for local node data (fast index), hash table for globalâ€‘toâ€‘local mapping, adjacency stored as perâ€‘node lists. Example: Dâ€‘Galois uses contiguous arrays for partitions + hash for crossâ€‘partition references.</p>

<p class="question">ğŸ”¹ CNN explained simply</p>
<p><strong>Answer:</strong> CNN learns hierarchical features from grid data (images) using convolutions (small filters slide, detect edges â†’ textures â†’ objects), parameter sharing (translation invariance), pooling (downsampling). This inspired Graph Neural Networks: graph convolutions aggregate neighbour info, message passing along edges, graph pooling for coarsening. Distributed GNN training on large graphs requires partitioning, handling boundary nodes, and efficient communication â€“ directly related to my interest.</p>

<h3>â””â”€ C. Programming (Python focus)</h3>

<p class="question">ğŸ”¹ Python proficiency â€“ NumPy, SciPy, PyTorch?</p>
<p><strong>Answer:</strong> Intermediateâ€‘advanced. Core: data structures, OOP, list comprehensions, generators. NumPy: vectorisation, broadcasting, linear algebra, random. SciPy: sparse matrices (critical for graphs), optimisation, stats. PyTorch/TensorFlow: custom layers, autograd, data loaders, distributed training basics. Graph libraries: NetworkX (prototyping), PyTorch Geometric (conceptual). Strengthening: mpi4py, CUDA, ParMETIS concepts â€“ all for distributed graph analytics.</p>

<p class="question">ğŸ”¹ List vs tuple â€“ differences</p>
<p><strong>Answer:</strong> List: mutable, square brackets, slower, overallocates memory. Tuple: immutable, parentheses, faster, less memory, can be dict keys. Use tuples for fixed data (coordinates, edges in prototyping), dict keys (caching). Use lists for dynamic sequences (adjacency builders, BFS queues). Memory efficiency matters for billionâ€‘scale graphs, so fixed structures often use tuples/arrays.</p>

<p class="question">ğŸ”¹ List comprehension â€“ what and when</p>
<p><strong>Answer:</strong> Concise syntax: [expr for item in iterable if cond]. Faster and clearer than manual loops for simple transformations/filters. Examples: filtering heavy edges, flattening adjacency, parsing graph input. For massive data, prefer generator expressions (lazy, memoryâ€‘efficient). In distributed pipelines, generator expressions avoid materialising large lists when streaming between nodes.</p>

<hr>
<div class="note">
    <strong>âœ Note:</strong> Sections 3 (deep dive into your projects) and 4 (thinkingâ€‘onâ€‘feet) should be prepared personally â€“ focus on your own code, past research, and reasoning out loud. Use this guide as a skeleton.
</div>

<!-- end -->
<p style="text-align:right; color:#4b5563;">â€” skoltech ACS Â· distributed graph analytics â€”</p>
</body>
</html>