<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Skoltech ACS interview prep – clean classic</title>
    <style>
        /* classic, clean, readable – no emojis, no clutter */
        body {
            background: #ffffff;
            font-family: 'Palatino', 'Georgia', 'Times New Roman', serif;
            font-size: 16px;
            line-height: 1.7;
            margin: 2rem auto;
            max-width: 950px;
            padding: 0 35px;
            color: #1e2a3a;
        }
        h1 {
            font-size: 2.2rem;
            font-weight: 500;
            border-bottom: 2px solid #2c3e50;
            padding-bottom: 0.3rem;
            margin-top: 2rem;
            margin-bottom: 1.5rem;
            letter-spacing: -0.3px;
            font-family: 'Helvetica Neue', 'Arial', sans-serif;
        }
        h2 {
            font-size: 1.7rem;
            font-weight: 500;
            margin: 2.5rem 0 1.2rem;
            background: #f0f4f9;
            padding: 0.5rem 1.2rem;
            border-left: 8px solid #1e3c5c;
            font-family: 'Helvetica Neue', sans-serif;
        }
        h3 {
            font-size: 1.4rem;
            font-weight: 600;
            margin: 2rem 0 1rem;
            color: #1e3c5c;
            border-bottom: 1px solid #b8ccdd;
            padding-bottom: 0.2rem;
        }
        h4 {
            font-size: 1.2rem;
            font-weight: 600;
            margin: 1.6rem 0 0.5rem;
            color: #2c3e50;
        }
        p {
            margin: 0.8rem 0;
            text-align: left;
        }
        ul, ol {
            margin: 0.5rem 0 1.2rem 1.8rem;
            padding-left: 0;
        }
        li {
            margin: 0.5rem 0;
        }
        pre {
            background: #f2f6fc;
            border-left: 5px solid #8ba3c0;
            padding: 1rem 1.5rem;
            overflow-x: auto;
            font-family: 'Courier New', 'Fira Code', monospace;
            font-size: 0.95rem;
            border-radius: 0 8px 8px 0;
            margin: 1.2rem 0;
        }
        code {
            background: #ecf2f9;
            padding: 0.2rem 0.5rem;
            border-radius: 5px;
            font-size: 0.95rem;
            font-family: 'Courier New', monospace;
        }
        hr {
            border: none;
            border-top: 2px dotted #a0b8d0;
            margin: 2.5rem 0;
        }
        .qa-block {
            margin: 1.5rem 0 2rem 0;
            padding-left: 1rem;
            border-left: 4px solid #d0deed;
        }
        .question {
            font-weight: 700;
            font-size: 1.2rem;
            color: #153e5c;
            margin-bottom: 0.5rem;
        }
        table {
            border-collapse: collapse;
            width: 85%;
            margin: 1.5rem 0;
            background: #fafdff;
            font-size: 1rem;
        }
        th {
            background: #e1eaf2;
            font-weight: 600;
            padding: 0.6rem;
        }
        td {
            border: 1px solid #b9cfdf;
            padding: 0.6rem;
        }
        .note-box {
            background: #f5f9fe;
            border: 1px solid #acc4db;
            padding: 1.2rem 2rem;
            border-radius: 12px;
            font-style: italic;
            margin: 2.2rem 0;
        }
        .tag {
            background: #1e3c5c;
            color: white;
            font-size: 0.75rem;
            padding: 0.2rem 0.9rem;
            border-radius: 20px;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            display: inline-block;
            margin-right: 0.6rem;
        }
    </style>
</head>
<body>

<h1>Skoltech ACS · interview preparation</h1>
<p><strong>Advanced Computational Science</strong> – distributed graph analytics / supercomputing</p>

<!-- ========== SECTION 1 – MOTIVATION ========== -->
<h2>1.  motivation & personal fit</h2>

<div class="qa-block">
    <div class="question">“Why ACS instead of a standard CS or Data Science master?”</div>
    <ul>
        <li>ACS sits at the intersection of high‑performance computing, applied mathematics, and large‑scale scientific data – not just software engineering or business analytics.</li>
        <li>I aim to build parallel and distributed algorithms that run on supercomputers, not only web or ML pipelines.</li>
        <li>Skoltech’s project‑based model and world‑class computing facilities are ideal for developing code that scales to thousands of nodes.</li>
    </ul>
</div>

<div class="qa-block">
    <div class="question">“Which research area within ACS attracts you most?”</div>
    <p><strong>Distributed Graph Analytics on Modern Supercomputing Architectures</strong> – because:</p>
    <ul>
        <li>Graphs are everywhere (social networks, biology, transport) and already at billion‑node scale.</li>
        <li>Modern supercomputers involve hybrid parallelism (MPI+X), heterogeneous nodes (CPU+GPU), and deep memory hierarchies – algorithms must adapt.</li>
        <li>Key questions I find exciting:
            <ul style="margin-top:0.3rem;">
                <li>How to partition massive graphs for load balance and low communication?</li>
                <li>Communication‑avoiding or asynchronous graph algorithms?</li>
                <li>Scaling graph neural networks across distributed memory?</li>
            </ul>
        </li>
        <li>I have followed frameworks like D‑Galois, Gemini, and MPI‑based graph libraries – eager to contribute.</li>
    </ul>
</div>

<div class="qa-block">
    <div class="question">“What would you like to work on in your master’s thesis?”</div>
    <ul>
        <li><strong>Communication‑avoiding graph algorithms</strong> – exploit mathematical structure to minimise data movement (new partitioning, asynchronous models, redundant computation vs communication).</li>
        <li><strong>Heterogeneous graph analytics</strong> – orchestrate CPUs and GPUs for irregular workloads, memory hierarchies, work distribution.</li>
        <li><strong>Scaling graph neural networks</strong> – distributed training, mini‑batch sampling across partitions, gradient synchronisation, handling graph dependencies.</li>
        <li>Open to refinement based on Skoltech’s ongoing projects and supervisor input – aim for a publication‑worthy contribution.</li>
    </ul>
</div>

<div class="qa-block">
    <div class="question">“Where do you see yourself in five years?”</div>
    <ul>
        <li>Pursuing a PhD in computational science or HPC at a top international institution – the ACS foundation (mathematics, parallel computing, research) is perfect preparation.</li>
        <li>Longer term: develop exascale algorithms for real‑world problems (scientific discovery, infrastructure optimisation), either in a national lab or deep‑tech R&amp;D.</li>
        <li>Also drawn to the intersection of HPC and AI – training massive models on supercomputers.</li>
    </ul>
</div>

<hr>

<!-- ========== SECTION 2 – TECHNICAL ========== -->
<h2>2.  technical fundamentals</h2>

<h3>mathematics – linear algebra</h3>

<div class="qa-block">
    <div class="question">Geometric meaning of eigenvectors and eigenvalues?</div>
    <ul>
        <li>Eigenvector = direction preserved under transformation (only scaled, not rotated). Eigenvalue = scaling factor.</li>
        <li>|λ| > 1 stretches, |λ| < 1 compresses, λ negative flips, λ = 0 collapses to null space.</li>
        <li>Graph example: eigenvectors of the Laplacian (Fiedler vector) give spectral embedding → used for partitioning. Number of zero eigenvalues = connected components.</li>
        <li>Data science: PCA eigenvectors = directions of maximum variance, eigenvalues = variance explained.</li>
    </ul>
</div>

<div class="qa-block">
    <div class="question">Matrix decomposition – why is SVD fundamental?</div>
    <ul>
        <li>Factorisation into simpler matrices (like prime factors). SVD: A = UΣVᵀ, with U,V orthonormal, Σ singular values.</li>
        <li>Why crucial:
            <ul>
                <li>Best low‑rank approximation (PCA, compression) – Eckart‑Young theorem.</li>
                <li>Condition number κ = σₘₐₓ / σₘᵢₙ.</li>
                <li>Stable pseudoinverse.</li>
                <li>Latent factors in recommendation, spectral graph analysis (community detection via adjacency SVD).</li>
            </ul>
        </li>
        <li>In my area: graph partitioning and embedding often use SVD or eigen‑decomposition.</li>
    </ul>
</div>

<div class="qa-block">
    <div class="question">Condition number – why care in numerical work?</div>
    <ul>
        <li>κ = σₘₐₓ / σₘᵢₙ measures sensitivity of Ax = b to input errors.</li>
        <li>Large κ → ill‑conditioned → small rounding or measurement noise blows up.</li>
        <li>Impacts: algorithm choice (direct vs iterative), need for preconditioning, trust in solutions.</li>
        <li>Graph Laplacians often become ill‑conditioned as size grows – affects iterative solvers for spectral methods.</li>
    </ul>
</div>

<h3>mathematics – calculus & optimisation</h3>

<div class="qa-block">
    <div class="question">Gradient descent, learning rate, stochastic gradient descent</div>
    <ul>
        <li>θ ← θ – η∇J(θ) – step down the gradient.</li>
        <li>Learning rate η: too small → slow or stuck; too large → overshoot or diverge.</li>
        <li>SGD: use one random sample (or mini‑batch) – noisy but fast, escapes shallow minima, online learning, implicit regularisation.</li>
        <li>Distributed training variants: data/model parallelism, sync/async – relevant for scaling GNNs.</li>
    </ul>
</div>

<div class="qa-block">
    <div class="question">Lagrange multiplier – idea and applications</div>
    <ul>
        <li>Optimise f(x) subject to g(x) = c → at optimum ∇f = λ∇g. λ = multiplier, measures sensitivity to constraint.</li>
        <li>Used in: SVM (dual, support vectors), PCA (variance under orthogonality), graph partitioning (min‑cut + balance → eigenvalue problem), duality for distributed optimisation, max‑ent distributions.</li>
    </ul>
</div>

<h3>mathematics – probability & statistics</h3>

<div class="qa-block">
    <div class="question">MLE vs MAP – difference?</div>
    <ul>
        <li>MLE: θ = argmax P(data|θ) – no prior, only data.</li>
        <li>MAP: θ = argmax P(data|θ) P(θ) – includes prior (Bayesian point estimate).</li>
        <li>MAP = MLE + regularisation (Gaussian prior → L2, Laplace → L1).</li>
        <li>In graph problems: priors can encode community size, smoothness, etc. As data grows, MAP → MLE.</li>
    </ul>
</div>

<div class="qa-block">
    <div class="question">Bias‑variance tradeoff explained</div>
    <ul>
        <li>Total error = bias² + variance + irreducible noise.</li>
        <li>Bias: systematic underfitting (simple model).</li>
        <li>Variance: sensitivity to training fluctuations (overfitting).</li>
        <li>Complexity ↑ → bias ↓, variance ↑ → optimum in between.</li>
        <li>In distributed graph analytics: GNN depth (bias vs over‑smoothing), graph sampling (variance), communication approximations (bias) to lower network variance.</li>
    </ul>
</div>

<div class="qa-block">
    <div class="question">Central Limit Theorem – why useful?</div>
    <ul>
        <li>Sample mean ≈ normal for large n, regardless of original distribution.</li>
        <li>Use cases: confidence intervals, hypothesis tests, Monte Carlo error bounds (∝ 1/√n), performance analysis (mean of runs), distributed aggregation, graph sampling error estimates.</li>
    </ul>
</div>

<h3>computer science & algorithms</h3>

<div class="qa-block">
    <div class="question">O(n log n) vs O(n²) – examples and relevance</div>
    <ul>
        <li>n = 1e6: n log n ≈ 20M ops (feasible), n² ≈ 1e12 ops (infeasible).</li>
        <li>O(n log n) algorithms: merge sort, heap sort, FFT, Dijkstra (binary heap), many sparse graph algorithms.</li>
        <li>O(n²): bubble sort, naive matrix multiply, Floyd‑Warshall, all‑pairs checks.</li>
        <li>For massive graphs, we need near‑linear in edges, plus communication‑aware distributed designs.</li>
    </ul>
</div>

<div class="qa-block">
    <div class="question">Hash table (dict) vs list – when to use?</div>
    <ul>
        <li>Hash table: O(1) lookup by key, uniqueness, set ops, counting, caching. Example: global→local node id mapping.</li>
        <li>List: order, index access, sequential iteration, memory compact. Example: local adjacency storage.</li>
        <li>Distributed frameworks often hybrid: array for local nodes (fast index) + hash for cross‑partition references.</li>
    </ul>
</div>

<div class="qa-block">
    <div class="question">CNN in simple terms – link to graphs</div>
    <ul>
        <li>CNN learns hierarchical features: convolutions (small filters slide), parameter sharing (translation invariance), pooling (downsampling).</li>
        <li>Inspires GNNs: graph convolutions (neighbour aggregation), message passing, graph pooling.</li>
        <li>Distributed GNN training needs partitioning, boundary handling, communication minimisation – connects to my interest.</li>
    </ul>
</div>

<h3>Python – practical</h3>

<div class="qa-block">
    <div class="question">Python proficiency – NumPy, SciPy, PyTorch?</div>
    <ul>
        <li>Core: data structures, OOP, generators, comprehensions.</li>
        <li>NumPy: vectorisation, broadcasting, linear algebra, random.</li>
        <li>SciPy: sparse matrices (key for graphs!), optimisation, stats.</li>
        <li>PyTorch/TF: custom layers, autograd, data loaders, basics of distributed training.</li>
        <li>Graph libs: NetworkX (prototype), PyG/DGL (conceptual).</li>
        <li>Strengthening: mpi4py, CUDA, ParMETIS concepts – for real distributed graph analytics.</li>
    </ul>
</div>

<div class="qa-block">
    <div class="question">List vs tuple – when to pick which?</div>
    <ul>
        <li>List: mutable, [ ], slower, overallocates – for dynamic sequences (adjacency builder, BFS queue).</li>
        <li>Tuple: immutable, ( ), faster, less memory, hashable – fixed coordinates, edge keys for caching.</li>
        <li>In billion‑scale graphs, fixed‑size tuples reduce memory overhead.</li>
    </ul>
</div>

<div class="qa-block">
    <div class="question">List comprehension – examples</div>
    <ul>
        <li>Syntax: [expr for item in iterable if cond] – concise and fast.</li>
        <li>Examples: filtering heavy edges, flattening adjacency, parsing graph input.</li>
        <li>For massive data, use generator expressions (lazy) to avoid large intermediates when streaming between nodes.</li>
    </ul>
</div>

<hr>

<!-- ========== SECTION 3 – QUICK REASONING ========== -->
<h2>3.  on‑the‑feet reasoning (practice aloud)</h2>

<div class="qa-block">
    <div class="question">“90% accuracy on a dataset with 90% class A, 10% class B – good model?”</div>
    <ul>
        <li>Not necessarily – baseline “always A” gives 90%.</li>
        <li>Need precision, recall, F1, confusion matrix.</li>
        <li>For imbalanced data, focus on recall for minority class and possibly ROC‑AUC or PR‑AUC.</li>
        <li>Example: cancer detection – missing 10% (false negatives) is unacceptable.</li>
    </ul>
</div>

<div class="qa-block">
    <div class="question">“How to detect anomalies in streaming network traffic?”</div>
    <ul>
        <li>Extract features (packet rate, byte rate, protocol mix, entropy of addresses).</li>
        <li>Statistical methods: moving average / EWMA, deviation > threshold.</li>
        <li>Unsupervised ML: isolation forest, one‑class SVM, autoencoders (reconstruction error).</li>
        <li>Streaming considerations: sliding windows, incremental models, alert cooldown.</li>
        <li>Multi‑stage: lightweight filter → if suspicious, deep ML → correlation engine.</li>
    </ul>
</div>

<div class="note-box">
    <strong>Note:</strong> Sections on deep project dive and further thinking‑on‑feet questions should be prepared personally – practice explaining your own projects with context, and always think out loud.
</div>

<hr>
<p style="text-align:right; color:#2f4f6f;">— prepared for Skoltech ACS / distributed graph analytics —</p>

</body>
</html>